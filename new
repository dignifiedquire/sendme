- arbitrary MAX_BLOBS_SIZE, basically how many files can we transfer
roughly 3MB, can transfer information about ~6000 files at once
4 bytes per char, about 100 chars in a file name + 32 bytes for the hash

- event passes a channel that returns vec[u8], read from that to write to
  whereever

- should i just simplify the 
- should we just store this in the main store & write it to a tempfile?

- We currently don't "pre-check" to see if we are missing certain hashes in the
  list. Should we pre-check? We currently don't skip & just give you everything
  we have without failing for a `NotFound`
- Do we write ListOfBlob data to tempfile, rather than creating a whole new
  store to store in mem?

- Choices made that are up for debate
  - to simplify, any `out` that is passed in is assumed to be the directory you
    want the file saved in.

DB choices that we can/should debate:
- There is currently a separate (mem) database for `ListOfBlobs` than the blobs
  themselves. It seems reasonable to me to instead store the `ListOfBlobs` in
  a tempfile, and store them in the same database as the blobs. This would
  streamline our DB story and also streamline the protocol code a bit.

Protocol choices made that we can/should debate:
- We don't "pre-check" the hashes to see if any are missing before starting
  a transfer. Trying to get to the actual "transfering" asap.
- As soon as a hash is `NotFound`, we end the transfer. We could continue
  trying to send other hashes in the list (if there are any).
- In the case of an error, we don't clean up any files that have already been 
  transfered successfully. This seems like a reasonable expectation to me, but
  open to be disagreed with.
- The `out` flag is now the expected directory you want to save the given file.
  We use the filename from the original path to name the file. In the case of
  stdin, this is the hash of the file.


General Question:
- The buffering story is pretty hard to follow currently. In `read_lp`
  & `read_lp_data` we do not resize the buffer (to size to the amount we expect
  to read) before reading.

  From that moment on, we always have to worry that there will be content left 
  over in the buffer. 

  It feels like less of a burden on the developer if we only attempt to read the 
  amount that we expect each time, and not have to worry that the buffer may contain 
  left over data we need to account for on the next read.

  However, it would probably be a performance loss to resize the
  buffer constantly. So I can understand why we wouldn't do it.

- Due to the way we are yielding readers to the event stream, we have to ensure
  that the user iterates through the whole stream until it is "done", or the
  last reader yielded will never be called to `await` it's task.

  I haven't figured out a way around this that doesn't fully change the way we
  are currently yielding the events. 


